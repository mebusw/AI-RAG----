import streamlit as st
import requests
from bs4 import BeautifulSoup
from bs4.element import Tag, NavigableString # Import NavigableString
import os
import re
import pandas as pd
from urllib.parse import urljoin, urlparse
import time
import hashlib # ç”¨äºç”Ÿæˆæ›´å”¯ä¸€çš„å›¾ç‰‡æ–‡ä»¶å

# --- é…ç½® ---
BASE_URL = 'https://www.ielts-mentor.com/writing-sample/writing-task-2'
OUTPUT_DIR = 'output'
MARKDOWN_DIR = os.path.join(OUTPUT_DIR, 'markdown')
IMAGES_DIR = os.path.join(OUTPUT_DIR, 'images')

## TODO
help="""
ielts-mentor.comç½‘ç«™ä¸Šçš„ä½œæ–‡ä¸º3ç±»ï¼š
- Aç±»å°ä½œæ–‡ã€‚ç›®å½•é“¾æ¥ https://www.ielts-mentor.com/writing-sample/academic-writing-task-1?start=0ï¼Œè¯¦æƒ…é“¾æ¥å½¢å¦‚ https://www.ielts-mentor.com/writing-sample/academic-writing-task-1/1001-numbers-of-male-and-female-research-students-at-us-university
- Gç±»å°ä½œæ–‡ï¼Œç›®å½•é“¾æ¥ https://www.ielts-mentor.com/writing-sample/gt-writing-task-1ï¼Œè¯¦æƒ…é“¾æ¥å½¢å¦‚ https://www.ielts-mentor.com/writing-sample/gt-writing-task-1/4117-you-started-in-your-present-job-two-years-ago
- å¤§ä½œæ–‡ï¼Œç›®å½•é“¾æ¥ https://www.ielts-mentor.com/writing-sample/writing-task-2ï¼Œè¯¦æƒ…é“¾æ¥å½¢å¦‚ https://www.ielts-mentor.com/writing-sample/writing-task-2/4083-success-in-life-comes-from-hard-work-dedication-and-motivation

ä¿å­˜ä¸‹æ¥çš„å›¾æ–‡ï¼Œéœ€è¦å¤„ç†ï¼Œæ¯”å¦‚æ¢å¤å›¾ç‰‡é“¾æ¥ï¼Œä»¥åŠåˆ©ç”¨æ­£åˆ™å»é™¤ 'Rating' å­—æ ·ä¹‹åæ— ç”¨è¯„è®º
"""

# è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
}

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(MARKDOWN_DIR, exist_ok=True)
os.makedirs(IMAGES_DIR, exist_ok=True)

# --- è¾…åŠ©å‡½æ•° ---
def slugify(text):
    """å°†æ–‡æœ¬è½¬æ¢ä¸ºé€‚åˆæ–‡ä»¶åçš„ slug"""
    text = str(text).strip().lower() # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
    text = re.sub(r'[^\w\s-]', '', text) # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[-\s]+', '-', text) # æ›¿æ¢ç©ºæ ¼å’Œå¤šä¸ªè¿å­—ç¬¦ä¸ºå•ä¸ªè¿å­—ç¬¦
    return text

def download_image(image_url, file_prefix):
    """ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„ï¼Œç”¨äº Markdown åµŒå…¥"""
    # ç¡®ä¿å›¾ç‰‡ URL æ˜¯ç»å¯¹è·¯å¾„
    if not image_url.startswith(('http://', 'https://')):
        image_url = urljoin(BASE_URL, image_url)

    try:
        response = requests.get(image_url, stream=True, timeout=10, headers=HEADERS)
        response.raise_for_status() # æ£€æŸ¥ HTTP é”™è¯¯

        # ä» URL è§£ææ–‡ä»¶å
        parsed_url = urlparse(image_url)
        path = parsed_url.path
        
        # å°è¯•ä»è·¯å¾„ä¸­è·å–æ–‡ä»¶åå’Œæ‰©å±•å
        filename_raw = os.path.basename(path)
        name, ext = os.path.splitext(filename_raw)

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ‰©å±•åï¼Œæˆ–è€…æ–‡ä»¶åçœ‹èµ·æ¥ä¸åƒæ–‡ä»¶ï¼ˆä¾‹å¦‚ç›®å½•åï¼‰ï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªå“ˆå¸Œå
        if not ext or len(ext) > 5 or not name: # ç®€å•åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ‰©å±•å
            # ä½¿ç”¨ URL çš„å“ˆå¸Œå€¼ä½œä¸ºæ–‡ä»¶åçš„ä¸€éƒ¨åˆ†ï¼Œç¡®ä¿å”¯ä¸€æ€§
            filename_hash = hashlib.md5(image_url.encode()).hexdigest()[:8]
            ext = '.png' # é»˜è®¤ä¸€ä¸ªå¸¸è§çš„å›¾ç‰‡æ‰©å±•å
            filename = f"{filename_hash}{ext}"
        else:
            filename = f"{name}{ext}"

        # ç»“åˆé¢˜ç›®slugå’Œå›¾ç‰‡åŸå§‹æ–‡ä»¶å
        # é™åˆ¶æ–‡ä»¶åçš„æ€»é•¿åº¦ï¼Œé¿å…æ“ä½œç³»ç»Ÿé™åˆ¶
        base_filename_prefix = slugify(file_prefix)[:50] # Use the file prefix (ID-title_slug)
        final_filename = f"{base_filename_prefix}_{filename}"
        
        image_path = os.path.join(IMAGES_DIR, final_filename)
        
        # ç¡®ä¿æ–‡ä»¶åæ˜¯å”¯ä¸€çš„ï¼Œé¿å…è¦†ç›–å·²å­˜åœ¨çš„åŒåæ–‡ä»¶
        counter = 1
        original_image_path = image_path
        while os.path.exists(image_path):
            name_part, ext_part = os.path.splitext(original_image_path)
            image_path = f"{name_part}_{counter}{ext_part}"
            counter += 1

        with open(image_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        # è¿”å›ç›¸å¯¹äº MARKDOWN_DIR çš„è·¯å¾„ï¼Œä»¥ä¾¿åœ¨ Markdown ä¸­æ­£ç¡®å¼•ç”¨
        return os.path.relpath(image_path, MARKDOWN_DIR) 
    except requests.exceptions.RequestException as e:
        st.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ ({image_url}): {e}")
        return None
    except Exception as e:
        st.warning(f"å¤„ç†å›¾ç‰‡å¤±è´¥ ({image_url}): {e}")
        return None

def scrape_ielts_samples_to_markdown(progress_bar_placeholder, status_text_placeholder):
    all_links = set()
    st.markdown("---") # åˆ†éš”çº¿
    st.subheader("ğŸ•¸ï¸ çˆ¬è™«è°ƒè¯•ä¿¡æ¯")
    debug_info_container = st.empty() # ç”¨äºåŠ¨æ€æ›´æ–°è°ƒè¯•ä¿¡æ¯

    all_links_file_path = os.path.join(OUTPUT_DIR, "all_links.md")
    links_loaded_from_file = False

    if os.path.exists(all_links_file_path):
        status_text_placeholder.text(f"å‘ç°é“¾æ¥æ–‡ä»¶: {all_links_file_path}ï¼Œå°è¯•è¯»å–...")
        debug_info_container.info(f"å°è¯•ä» {all_links_file_path} è¯»å–é“¾æ¥...")
        try:
            with open(all_links_file_path, 'r', encoding='utf-8') as f:
                loaded_links = [line.strip() for line in f if line.strip() and line.strip().startswith('http')]
            if loaded_links:
                all_links = set(loaded_links)
                links_loaded_from_file = True
                st.sidebar.info(f"å·²ä» {all_links_file_path} åŠ è½½ {len(all_links)} ä¸ªé“¾æ¥ã€‚")
                debug_info_container.success(f"æˆåŠŸä» {all_links_file_path} åŠ è½½ {len(all_links)} ä¸ªé“¾æ¥ã€‚")
            else:
                st.sidebar.warning(f"{all_links_file_path} ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆé“¾æ¥ã€‚å°†å°è¯•åœ¨çº¿æŠ“å–ã€‚")
                debug_info_container.warning(f"{all_links_file_path} ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆé“¾æ¥ã€‚å°†å°è¯•åœ¨çº¿æŠ“å–ã€‚")
        except Exception as e:
            st.sidebar.error(f"è¯»å– {all_links_file_path} å¤±è´¥: {e}ã€‚å°†å°è¯•åœ¨çº¿æŠ“å–ã€‚")
            debug_info_container.error(f"è¯»å– {all_links_file_path} å¤±è´¥: {e}ã€‚å°†å°è¯•åœ¨çº¿æŠ“å–ã€‚")

    if not links_loaded_from_file:
        status_text_placeholder.text("æ­£åœ¨åœ¨çº¿æŠ“å–é¢˜ç›®é“¾æ¥...")
        current_page = 0
        all_links = set() #ç¡®ä¿ä»å¤´å¼€å§‹æ”¶é›†

        while True:
            url = f"{BASE_URL}?start={current_page * 20}"
            debug_info_container.info(f"æ­£åœ¨å°è¯•è®¿é—®åˆ†é¡µ URL: `{url}`")
            status_text_placeholder.text(f"æ­£åœ¨è®¿é—®åˆ†é¡µ: {url}")
            
            page_links_found_count = 0
            new_links_added_count = 0

            try:
                response = requests.get(url, timeout=10, headers=HEADERS)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                debug_info_container.code(f"é¡µé¢å†…å®¹ç‰‡æ®µ (URL: {url}):\n{response.text[:500]}...", language='html')

                links_on_page_elements = soup.select('form#adminForm table.category td.list-title a')
                page_links_found_count = len(links_on_page_elements)
                debug_info_container.info(f"åœ¨ `{url}` é¡µé¢æ‰¾åˆ° `{page_links_found_count}` ä¸ª `form#adminForm table.category td.list-title a` å…ƒç´ ã€‚")
                
                if not links_on_page_elements and current_page > 0:
                    st.info(f"åœ¨ `{url}` æœªæ‰¾åˆ°ä»»ä½•é“¾æ¥å…ƒç´ ï¼Œå¯èƒ½å·²åˆ°è¾¾æœ€åä¸€é¡µã€‚åœæ­¢åˆ†é¡µéå†ã€‚")
                    break

                found_valid_links_on_this_page = []
                for link_tag in links_on_page_elements:
                    href = link_tag.get('href')
                    if href and '/writing-sample/writing-task-2/' in href and not href.endswith('/writing-task-2'):
                        full_url = urljoin("https://www.ielts-mentor.com", href)
                        if full_url not in all_links:
                            all_links.add(full_url)
                            new_links_added_count += 1
                            found_valid_links_on_this_page.append(full_url)
                
                if found_valid_links_on_this_page:
                    debug_info_container.write(f"æœ¬é¡µæ–°å¢é¢˜ç›®é“¾æ¥ ({new_links_added_count} ä¸ª):")
                    for link_item in found_valid_links_on_this_page: # Renamed to avoid conflict
                        debug_info_container.text(f"- {link_item}")
                else:
                    debug_info_container.info("æœ¬é¡µæœªå‘ç°æ–°çš„æœ‰æ•ˆé¢˜ç›®é“¾æ¥ã€‚")

                if new_links_added_count == 0 and page_links_found_count == 0 and current_page > 0:
                     st.info(f"åœ¨ `{url}` æœªæ‰¾åˆ°ä»»ä½•æ–°çš„é¢˜ç›®é“¾æ¥ï¼Œä¸”é¡µé¢å…ƒç´ ä¸ºç©ºã€‚åœæ­¢åˆ†é¡µéå†ã€‚")
                     break

                current_page += 1
                st.sidebar.markdown(f'<p class="text-sm text-gray-600">å·²å‘ç° <strong>{len(all_links)}</strong> ä¸ªé¢˜ç›®é“¾æ¥ (æœ¬é¡µæ–°å¢: {new_links_added_count})</p>', unsafe_allow_html=True)
                time.sleep(2)

            except requests.exceptions.RequestException as e:
                st.error(f"è®¿é—®åˆ†é¡µ `{url}` æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
                debug_info_container.error(f"ç½‘ç»œé”™è¯¯è¯¦æƒ…: {e}")
                break
            except Exception as e:
                st.error(f"å¤„ç†åˆ†é¡µ `{url}` æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                debug_info_container.error(f"æœªçŸ¥é”™è¯¯è¯¦æƒ…: {e}")
                break
        
        # ä¿å­˜æ–°æŠ“å–åˆ°çš„é“¾æ¥
        if all_links:
            try:
                with open(all_links_file_path, 'w', encoding='utf-8') as f:
                    for link_item in sorted(list(all_links)):
                        f.write(link_item + '\n')
                st.sidebar.info(f"æ–°æŠ“å–çš„ {len(all_links)} ä¸ªé¢˜ç›®é“¾æ¥å·²ä¿å­˜åˆ°: {all_links_file_path}")
                debug_info_container.success(f"æ–°æŠ“å–çš„ {len(all_links)} ä¸ªé¢˜ç›®é“¾æ¥å·²ä¿å­˜åˆ°: {all_links_file_path}")
            except Exception as e:
                st.sidebar.error(f"ä¿å­˜ {all_links_file_path} å¤±è´¥: {e}")
                debug_info_container.error(f"ä¿å­˜ {all_links_file_path} å¤±è´¥: {e}")
        elif not links_loaded_from_file: # å¦‚æœåœ¨çº¿æŠ“å–ä¹Ÿæ²¡æœ‰é“¾æ¥
             st.sidebar.warning("åœ¨çº¿æŠ“å–æœªå‘ç°ä»»ä½•é“¾æ¥ã€‚")
             debug_info_container.warning("åœ¨çº¿æŠ“å–æœªå‘ç°ä»»ä½•é“¾æ¥ã€‚çˆ¬è™«å°†ä¸ä¼šå¤„ç†ä»»ä½•è¯¦æƒ…é¡µé¢ã€‚")

    if not all_links:
        status_text_placeholder.warning("æ²¡æœ‰å¯ä¾›æŠ“å–çš„é“¾æ¥ã€‚è¯·æ£€æŸ¥é“¾æ¥æ–‡ä»¶æˆ–ç½‘ç»œè¿æ¥ã€‚")
        debug_info_container.warning("æœ€ç»ˆé“¾æ¥åˆ—è¡¨ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå†…å®¹æŠ“å–ã€‚")
        return

    status_text_placeholder.text(f"æ€»å…±å‘ç°/åŠ è½½ {len(all_links)} ä¸ªé¢˜ç›®é“¾æ¥ï¼Œå¼€å§‹æŠ“å–è¯¦ç»†å†…å®¹...")
    debug_info_container.info(f"æœ€ç»ˆç¡®è®¤çš„æ€»é¢˜ç›®é“¾æ¥æ•°: {len(all_links)}")

    total_links = len(all_links)
    crawled_count = 0

    # æ’åºé“¾æ¥ä»¥ç¡®ä¿æ¯æ¬¡è¿è¡Œçš„æŠ“å–é¡ºåºä¸€è‡´ï¼Œå¹¶æé«˜å¯è°ƒè¯•æ€§
    for i, link_item in enumerate(sorted(list(all_links))): # Renamed to avoid conflict
        crawled_count += 1
        progress_percentage = (crawled_count / total_links) * 100
        progress_bar_placeholder.progress(int(progress_percentage))
        status_text_placeholder.text(f"æ­£åœ¨æŠ“å– ({crawled_count}/{total_links}): {link_item}")
        
        debug_info_container.info(f"æ­£åœ¨æŠ“å–è¯¦æƒ…é¡µé¢: `{link_item}`")

        try:
            parsed_url = urlparse(link_item)
            # Use the last part of the URL path for the markdown filename
            url_path_basename = os.path.basename(parsed_url.path)

            # Fallback if basename is empty or just a slash (e.g. for root paths if they were ever encountered)
            if not url_path_basename or url_path_basename == '/':
                url_path_basename = hashlib.md5(link_item.encode()).hexdigest()[:16] # Use a hash of the URL
                debug_info_container.warning(f"URL path basename for {link_item} was empty/invalid. Using hash: {url_path_basename}")

            final_markdown_filename = os.path.join(MARKDOWN_DIR, f"{url_path_basename}.md")

            # Check if the markdown file already exists using the new naming convention
            if os.path.exists(final_markdown_filename):
                st.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {final_markdown_filename}")
                debug_info_container.info(f"æ–‡ä»¶ `{final_markdown_filename}` å·²å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
                continue

            response = requests.get(link_item, timeout=10, headers=HEADERS)
            response.raise_for_status()
            page_soup = BeautifulSoup(response.text, 'html.parser')

            # Get title for page metadata and for constructing the image file prefix
            title_tag = page_soup.select_one('h1.title')
            title = title_tag.get_text(strip=True) if title_tag else "No Title Found"
            title_slug_from_page = slugify(title)

            # For image file prefix: parse ID from url_path_basename (if pattern exists) and use title_slug.
            # This ID helps in making image names more specific and tied to content.
            id_match_for_image = re.match(r'(\d+)-', url_path_basename) # url_path_basename is the base for the markdown filename
            file_id_for_image_prefix = id_match_for_image.group(1) if id_match_for_image else slugify(link_item)[:8] # Fallback for image ID part

            # Try to find article.item-page first, then div.item-page as a fallback
            content_container = page_soup.find('article', class_='item-page')
            if not content_container:
                content_container = page_soup.find('div', class_='item-page')
            
            markdown_content_parts = []
            if content_container:
                extravote_marker_div = content_container.find('div', class_='size-1 extravote')
                if extravote_marker_div:
                    debug_info_container.info(f"Found 'extravote' marker div ('div.size-1.extravote'). Content will be extracted before this element.")
                else:
                    debug_info_container.warning(f"Could not find 'extravote' marker div ('div.size-1.extravote'). Will attempt to process all content within the item-page container.")

                # 1. Collect relevant child nodes before the extravote marker
                nodes_before_extravote = []
                for node in content_container.children:
                    if not isinstance(node, (Tag, NavigableString)): # Use imported Tag and NavigableString
                        continue
                    if extravote_marker_div and node is extravote_marker_div:
                        debug_info_container.info("Reached 'extravote' div while collecting nodes. Stopping collection.")
                        break
                    nodes_before_extravote.append(node)

                if not nodes_before_extravote:
                    st.warning(f"No content found before 'extravote' div (or 'extravote' div was at the very beginning) for {link_item}")
                    debug_info_container.warning(f"No nodes collected before 'extravote' for {link_item}. Markdown file might be empty or contain only title/link.")
                
                # 2. Create a temporary soup with copies of these nodes to process them (images, then markdown)
                temp_processing_soup_parent = BeautifulSoup('<div></div>', 'html.parser').div
                for node_to_copy in nodes_before_extravote:
                    temp_processing_soup_parent.append(node_to_copy.__copy__()) # Append copies to isolate modifications

                # 3. Process images globally within this temporary soup (which contains content before extravote)
                img_tags_in_temp_soup = temp_processing_soup_parent.find_all('img')
                if img_tags_in_temp_soup:
                    debug_info_container.info(f"Processing {len(img_tags_in_temp_soup)} image(s) from collected content for {link_item}.")
                for img in img_tags_in_temp_soup:
                    src = img.get('src')
                    if src and not src.startswith('data:'): # Ensure it's a downloadable link
                        alt_text = img.get('alt', 'Image') # Get alt text or use default
                        local_image_path = download_image(src, f"{file_id_for_image_prefix}-{title_slug_from_page}")
                        if local_image_path:
                            debug_info_container.info(f"Image `{src}` downloaded, embedding as `{local_image_path}`.")
                            # Replace img tag with a placeholder string that includes all necessary info
                            img.replace_with(f"@@IMAGE_PLACEHOLDER@@{alt_text}@@{local_image_path}@@{src}@@")
                        else:
                            debug_info_container.warning(f"Image `{src}` download failed, removing from content.")
                            img.extract() # Remove the tag if download fails
                    else: # data URI, empty src, or other non-downloadable src
                        debug_info_container.info(f"Invalid, Base64, or non-downloadable image (src: '{src if src else 'None'}') removed.")
                        img.extract() # Remove the tag
                
                # 4. Convert children of this temporary, image-processed soup to Markdown
                # Collect text and placeholder strings
                raw_content_parts = []
                for element in temp_processing_soup_parent.children: # Added missing loop
                    if isinstance(element, Tag): # Use the imported Tag class directly
                        if element.name == 'p':
                            # Get text, ensuring spaces between inline elements, then strip leading/trailing whitespace
                            p_text = element.get_text(separator=' ', strip=True)
                            if p_text: # Only add if paragraph has content
                                raw_content_parts.append(p_text + '\n\n') # Changed to raw_content_parts
                        elif element.name and element.name.startswith('h'):
                            level = int(element.name[1]) # h1, h2, etc.
                            raw_content_parts.append('#' * level + ' ' + element.get_text(strip=True) + '\n\n')
                        elif element.name == 'ul' or element.name == 'ol':
                            list_items_md = []
                            for li in element.find_all('li', recursive=False): # Only direct children li
                                list_items_md.append(f"- {li.get_text(strip=True)}")
                            if list_items_md: # Only add list if it has items
                                raw_content_parts.append('\n'.join(list_items_md) + '\n\n') # Changed to raw_content_parts
                        elif element.name == 'table':
                            table_markdown = []
                            headers = [th.get_text(strip=True) for th in element.find_all('th')]
                            if headers:
                                table_markdown.append('| ' + ' | '.join(headers) + ' |')
                                table_markdown.append('|' + '----|' * len(headers))
                            
                            for row_tag in element.find_all('tr', recursive=False): # Only direct children tr
                                cells = [td.get_text(strip=True) for td in row_tag.find_all('td')]
                                if cells: # Only add row if it has cells
                                    table_markdown.append('| ' + ' | '.join(cells) + ' |')
                            if table_markdown: # Only add table if it has some content
                                raw_content_parts.append('\n'.join(table_markdown) + '\n\n') # Changed to raw_content_parts
                        elif element.name == 'strong':
                            # This handles <strong> tags that are direct children.
                            # Text within <p><strong>text</strong></p> is handled by p.get_text().
                            strong_text = element.get_text(strip=True)
                            if strong_text: raw_content_parts.append(f"**{strong_text}**")
                        elif element.name == 'em':
                            em_text = element.get_text(strip=True)
                            if em_text: raw_content_parts.append(f"*{em_text}*")
                        elif element.name == 'a':
                            href = element.get('href')
                            text = element.get_text(strip=True)
                            if href: # For links, typically we want the text, and the URL is already captured or not needed for this conversion
                                raw_content_parts.append(text) # Keep only text, or format as [text](href) if desired
                            else: # If 'a' tag has no href but has text
                                if text: raw_content_parts.append(text)
                        # Removed obsolete/misplaced block for "![Image]" handling

                    elif isinstance(element, NavigableString): # Process direct text nodes, use imported NavigableString
                        stripped_text = element.strip()
                        if stripped_text:
                            raw_content_parts.append(stripped_text + '\n\n')
                    elif isinstance(element, str) and element.startswith("@@IMAGE_PLACEHOLDER@@"): # Handle the placeholder string
                         raw_content_parts.append(element)

            else:
                st.warning(f"æœªèƒ½æ‰¾åˆ°å†…å®¹åŒºåŸŸ ('article.item-page' or 'div.item-page'): {link_item}")
                debug_info_container.warning(f"æœªèƒ½æ‰¾åˆ°å†…å®¹åŒºåŸŸ `article.item-page` or `div.item-page`ï¼Œè·³è¿‡æ­¤é“¾æ¥: {link_item}")
                continue

            # Join raw parts and replace image placeholders
            raw_markdown_content = "".join(raw_content_parts)
            final_markdown_content = re.sub(r"@@IMAGE_PLACEHOLDER@@(.*?)@@(.*?)@@(.*?)@@", 
                                            r"!\1(\2)\n\nOriginal Image Source\n\n", raw_markdown_content)

            final_markdown = f"# {title}\n\n"
            final_markdown += f"åŸæ–‡é“¾æ¥: [{link_item}]({link_item})\n\n" # Corrected link format
            final_markdown += final_markdown_content.strip() # Use processed content

            with open(final_markdown_filename, 'w', encoding='utf-8') as f:
                f.write(final_markdown)
            
            st.success(f"å·²ä¿å­˜: {final_markdown_filename}")
            debug_info_container.success(f"å·²æˆåŠŸä¿å­˜ `{final_markdown_filename}`ã€‚")

        except requests.exceptions.RequestException as e:
            st.error(f"æŠ“å– `{link_item}` æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
            debug_info_container.error(f"æŠ“å–è¯¦æƒ…é¡µé¢ `{link_item}` å¤±è´¥: ç½‘ç»œé”™è¯¯: {e}")
        except Exception as e: # Catch any other unexpected errors
            st.error(f"å¤„ç† `{link_item}` æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            debug_info_container.error(f"å¤„ç†è¯¦æƒ…é¡µé¢ `{link_item}` å¤±è´¥: {e}")
        time.sleep(2) # å¢åŠ ç¤¼è²Œæ€§å»¶è¿Ÿ

    status_text_placeholder.success("çˆ¬è™«å®Œæˆï¼æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ã€‚")
    debug_info_container.success("çˆ¬è™«æ‰§è¡Œå®Œæ¯•ï¼")

def load_local_data():
    """ä»æœ¬åœ° Markdown æ–‡ä»¶åŠ è½½æ•°æ®ï¼Œç”¨äºæ˜¾ç¤ºåœ¨ Streamlit ä¸­"""
    data = []
    for filename in os.listdir(MARKDOWN_DIR):
        if filename.endswith(".md"):
            filepath = os.path.join(MARKDOWN_DIR, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    title_match = re.search(r'# (.+?)\n', content)
                    link_match = re.search(r'\[åŸæ–‡é“¾æ¥]\((.+?)\)', content)
                    
                    title = title_match.group(1).strip() if title_match else filename.replace('.md', '').replace('-', ' ').title()
                    source_url = link_match.group(1).strip() if link_match else "N/A"
                    
                    image_paths_in_md = re.findall(r'!\[.*?]\((.+?)\)', content)
                    
                    data.append({
                        "filename": filename,
                        "title": title,
                        "source_url": source_url,
                        "content": content,
                        "image_paths": image_paths_in_md # ä¿å­˜ä» Markdown ä¸­è§£æå‡ºçš„ç›¸å¯¹è·¯å¾„
                    })
            except Exception as e:
                st.warning(f"åŠ è½½æ–‡ä»¶ {filename} å¤±è´¥: {e}")
    return pd.DataFrame(data)

# --- Streamlit UI ---
st.set_page_config(
    page_title="IELTS Academic Writing Task 1 çˆ¬è™«",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ä½¿ç”¨ Tailwind CSS CDN
st.markdown("""
<head>
  <script src="https://cdn.tailwindcss.com"></script>
</head>
""", unsafe_allow_html=True)

st.markdown("""
<style>
.stApp {
    background-color: #f8fafc; /* Tailwind gray-50 */
}
.stButton>button {
    background-color: #3b82f6; /* Tailwind blue-500 */
    color: white;
    padding: 0.75rem 1.5rem;
    border-radius: 0.5rem;
    font-weight: 600;
    transition: background-color 0.2s;
}
.stButton>button:hover {
    background-color: #2563eb; /* Tailwind blue-600 */
}
.stProgress > div > div {
    background-color: #22c55e; /* Tailwind green-500 */
}
.stSuccess {
    background-color: #d1fae5; /* Tailwind green-100 */
    color: #065f46; /* Tailwind green-800 */
    border-radius: 0.5rem;
    padding: 1rem;
}
.stWarning {
    background-color: #fffbeb; /* Tailwind yellow-100 */
    color: #92400e; /* Tailwind yellow-800 */
    border-radius: 0.5rem;
    padding: 1rem;
}
.stError {
    background-color: #fee2e2; /* Tailwind red-100 */
    color: #991b1b; /* Tailwind red-800 */
    border-radius: 0.5rem;
    padding: 1rem;
}
.stDataFrame {
    border-radius: 0.5rem;
    overflow: hidden;
}
/* For Markdown rendered images */
.stMarkdown img {
    max-width: 100%;
    height: auto;
    border-radius: 0.5rem;
    box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
}
</style>
""", unsafe_allow_html=True)


st.markdown('<h1 class="text-4xl font-bold text-center text-gray-800 mb-6">IELTS Academic Writing Task 1 é¢˜ç›®çˆ¬è™«</h1>', unsafe_allow_html=True)

st.sidebar.header("æ§åˆ¶é¢æ¿")
if st.sidebar.button("å¯åŠ¨çˆ¬è™«", key="start_crawler", help="ç‚¹å‡»å¼€å§‹æŠ“å–é›…æ€å†™ä½œTask 1é¢˜ç›®å¹¶ä¿å­˜åˆ°æœ¬åœ°"):
    st.sidebar.markdown('<p class="text-lg font-semibold text-blue-600">çˆ¬è™«çŠ¶æ€:</p>', unsafe_allow_html=True)
    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()
    
    scrape_ielts_samples_to_markdown(progress_bar, status_text)

st.markdown('<hr class="my-8">', unsafe_allow_html=True)

st.markdown('<h2 class="text-3xl font-semibold text-gray-700 mb-4">å·²æŠ“å–æ•°æ®</h2>', unsafe_allow_html=True)

# ä»æœ¬åœ°åŠ è½½æ•°æ®
df = load_local_data()

if not df.empty:
    st.write(f"æœ¬åœ°å…±æœ‰ {len(df)} æ¡æ•°æ®ã€‚")

    # æ•°æ®è¿‡æ»¤å’Œæœç´¢
    search_term = st.text_input("æœç´¢é¢˜ç›® (æ ‡é¢˜æˆ–å†…å®¹ä¸­åŒ…å«):", "")
    if search_term:
        df_filtered = df[df['title'].str.contains(search_term, case=False, na=False) | 
                         df['content'].str.contains(search_term, case=False, na=False)]
    else:
        df_filtered = df

    if not df_filtered.empty:
        st.dataframe(df_filtered[['title', 'source_url', 'filename']].style.set_properties(**{'font-size': '1.0em'}))

        # å¯¼å‡ºæ•°æ®æ¦‚è§ˆ
        st.download_button(
            label="ä¸‹è½½æ•°æ®æ¦‚è§ˆä¸º CSV",
            data=df_filtered[['title', 'source_url', 'filename']].to_csv(index=False).encode('utf-8'),
            file_name="ielts_writing_task1_overview.csv",
            mime="text/csv",
            help="ä¸‹è½½æœ¬åœ°å·²æŠ“å–é¢˜ç›®çš„æ¦‚è§ˆä¿¡æ¯"
        )
        
        st.markdown('<h3 class="text-2xl font-semibold text-gray-700 mt-8 mb-4">è¯¦ç»†å†…å®¹é¢„è§ˆ</h3>', unsafe_allow_html=True)
        
        # ä½¿ç”¨ selectbox æ˜¾ç¤ºæ ‡é¢˜ï¼Œæ–¹ä¾¿æŸ¥çœ‹è¯¦æƒ…
        selected_title = st.selectbox("é€‰æ‹©ä¸€ä¸ªé¢˜ç›®æŸ¥çœ‹è¯¦ç»†å†…å®¹:", df_filtered['title'].tolist())
        
        if selected_title:
            selected_sample = df_filtered[df_filtered['title'] == selected_title].iloc[0]
            st.markdown(f'<h4 class="text-xl font-bold text-gray-800 mt-4">{selected_sample["title"]}</h4>', unsafe_allow_html=True)
            st.markdown(f'<p class="text-gray-600 text-sm">æ¥æº: <a href="{selected_sample["source_url"]}" target="_blank" class="text-blue-500 hover:underline">{selected_sample["source_url"]}</a></p>', unsafe_allow_html=True)
            
            st.markdown('<h5 class="text-lg font-semibold text-gray-700 mt-4">å†…å®¹:</h5>', unsafe_allow_html=True)
            st.markdown(selected_sample['content'], unsafe_allow_html=True)
            
            if selected_sample['image_paths']:
                st.markdown('<h5 class="text-lg font-semibold text-gray-700 mt-4">å›¾ç‰‡ (æœ¬åœ°è·¯å¾„):</h5>', unsafe_allow_html=True)
                for img_path in selected_sample['image_paths']:
                    st.markdown(f'- `{img_path}`')
            else:
                st.info("æ­¤é¢˜ç›®æ²¡æœ‰å›¾ç‰‡æˆ–å›¾ç‰‡æœªæˆåŠŸä¸‹è½½/åµŒå…¥ã€‚")

    else:
        st.info("æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæœç´¢æ¡ä»¶çš„é¢˜ç›®ã€‚")
else:
    st.info("æœ¬åœ°è¿˜æ²¡æœ‰é›…æ€å†™ä½œé¢˜ç›®ã€‚è¯·ç‚¹å‡»å·¦ä¾§æŒ‰é’®å¯åŠ¨çˆ¬è™«ã€‚")