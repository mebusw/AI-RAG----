##pip install python-dotenv langchain chromadb tiktoken sentence-transformers

import os
from dotenv import load_dotenv
import ollama
import base64
from langchain_chroma import Chroma
from langchain_openai import OpenAI, OpenAIEmbeddings, ChatOpenAI
from langchain.chains import create_retrieval_chain, RetrievalQA
from pydantic import BaseModel, Field
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
import requests
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import logging
# logging.basicConfig(level=logging.DEBUG)

# Step 0: åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
OPENAI_API_URL = os.getenv("OPENAI_API_URL", "https://ark.cn-beijing.volces.com/api/v3")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


# Step 1.1 ä»PDFæ–‡æ¡£æèµ·æ–‡æœ¬
# å…ˆå°†æ–‡æ¡£è½¬æ¢ä¸ºå›¾åƒï¼Œç„¶åï¼Œä½¿ç”¨Tesseractè¯†åˆ«å›¾åƒä¸­çš„æ–‡æœ¬ã€‚Tesseractæ˜¯HPåœ¨1985å¹´å¼€å‘çš„ä¸»è¦OCRç³»ç»Ÿï¼Œç›®å‰ç”±è°·æ­Œç»´æŠ¤ã€‚
def handle_PDFs():
    import pdf2image #1.17.0
    doc_img = pdf2image.convert_from_path("data/doc_nvidia.pdf", dpi=300)

    import pytesseract #0.3.10
    doc_txt = []
    for page in doc_img:
        text = pytesseract.image_to_string(page)
        doc_txt.append(text)

# Step 1.2: åŠ è½½å’Œé¢„å¤„ç†æ–‡æœ¬
def load_documents():
    loader = TextLoader("csm_course_outline.txt")
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(documents)

    ## å¯é€‰åœ°ï¼Œä½¿ç”¨ç›®å½•ä¸ºæ®µè½æ·»åŠ æ ‡ç­¾
    def add_metadata(doc_txt=[]):
        title_map = { 
            "4-12" : "Business", 
            "13-33" : "Risk Factors",
            "34-44" : "Financials",
            "45-46" : "Directors",
            "47-83" : "Data" 
        }
        lst_docs, lst_ids, lst_metadata = [], [], []
        for n, page in enumerate(doc_txt):
            try:
                ## è·å–æ ‡é¢˜
                title = [v for k,v in title_map.items() if n in range(int(k.split("-")[0]), int(k.split("-")[1])+1)][0]
                ## æ¸…ç†é¡µé¢
                page = page.replace("Table of Contents","")
                ## è·å–æ®µè½
                for i,p in enumerate(page.split('\n\n')):
                    if len(p.strip())>5: 
                        lst_docs.append(p.strip())
                        lst_ids.append(str(n)+"_"+str(i))
                        lst_metadata.append({"title":title})
            except:
                continue
    
    ## å¯é€‰åœ°ï¼Œå…ƒæ•°æ®å¢å¼ºå¯ä»¥æ˜¾è‘—æé«˜æ–‡æ¡£æ£€ç´¢æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨Phi3å°†æ¯ä¸ªæ®µè½æ€»ç»“ä¸ºå‡ ä¸ªå…³é”®è¯ã€‚
    def keyword_generator(p, top=3):
        prompt = "summarize the following paragraph in 3 keywords separated by , : "
        res = ollama.generate(model="phi3", prompt=prompt)["response"]
        return res.replace("\n", " ").strip()

    
    return split_docs

# Step 1.3: åŠ è½½å’Œé¢„å¤„ç†å›¾åƒå›¾è¡¨
# æœ¬å‡½æ•°æ˜¯è°ƒç”¨ollamaæŸä¸ªæ¨¡å‹æ¥æè¿°å›¾åƒæˆ–å›¾è¡¨
def load_images():
    def encode_image(path):
        with open(path, "rb") as file:
            return base64.b64encode(file.read()).decode('utf-8')

    img = encode_image("/path/to/image.jpg")
    prompt = "describe the image"
    res = ollama.generate(model="llava", prompt=prompt, images=[img])["response"] ##æ³¨æ„è¿™é‡Œçš„æç¤ºè¯å‚æ•°æ˜¯promptï¼Œä¸æ˜¯messages
    print(res)

    img = encode_image("/path/to/diagram.jpg")
    prompt = "Describe the image in detail. Be specific about graphs, such as bar plots, line graphs, etc."
    res = ollama.generate(model="llava", prompt=prompt, images=[img])["response"] ##æ³¨æ„è¿™é‡Œçš„æç¤ºè¯å‚æ•°æ˜¯promptï¼Œä¸æ˜¯messages
    print(res)



# Step 2: åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“
def load_or_create_vectorstore():
    embeddings = OpenAIEmbeddings(
                    api_key=OPENAI_API_KEY, 
                    model="ep-20250104171017-p8sfd", 
                    base_url=OPENAI_API_URL, 
                    tiktoken_enabled=False
                )

    if os.path.exists("local_db"):
        print("Loading existing vector database...")
        return Chroma(persist_directory="local_db/", embedding_function=embeddings)
    else:
        print("Creating new vector database...")
        documents = load_documents()
        return Chroma.from_documents(documents, embeddings, persist_directory="local_db/")

# Step 3: æ„å»ºä»£ç†
def build_agent(vectorstore):
    retriever = vectorstore.as_retriever()
    llm = ChatOpenAI(temperature=0, api_key=OPENAI_API_KEY, model="ep-20250103223903-7kzhd", base_url=OPENAI_API_URL)
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
    return qa_chain
    
# Step 4: ä¸»å‡½æ•°ï¼Œè¿è¡Œä»£ç†
def main():

    def _main_loop():
        vectorstore = load_or_create_vectorstore()
        agent = build_agent(vectorstore)
        while True:
            print("\nAI Agent is ready. Type your questions (type 'exit' to quit).")
            query = input(">>> You: ")
            if query.lower() == "exit":
                break
            
            result = agent.invoke({"query": query})
            print(">>> AI: ", result["result"])
            print("Sources:", [doc.metadata["source"] for doc in result["source_documents"]])
    # _main_loop()
    buildUI()


# Step 4A: Streamlitæ˜¯æ„å»ºå¿«é€ŸWebåº”ç”¨ç¨‹åºæœ€å¸¸ç”¨çš„Pythonåº“ï¼Œå› ä¸ºå®ƒé€šè¿‡å…¶æµå¼åŠŸèƒ½ç®€åŒ–äº†NLPåº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚é¦–å…ˆï¼Œå®šä¹‰å¸ƒå±€ï¼šæˆ‘çš„å±å¹•åº”æœ‰ä¸€ä¸ªä¾§è¾¹æ ï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶ä¸­æŸ¥çœ‹èŠå¤©å†å²è®°å½•ã€‚
import streamlit as st
def buildUI():
    ## å¸ƒå±€
    st.title("Write your questions")
    st.sidebar.title("Chat History")

    app = st.session_state
    if 'messages' not in app:
        app['messages'] = [{"role": "assistant", "content": "I'm ready to retrieve information"}]
    if 'history' not in app:
        app['history'] = []
    if 'full_response' not in app:
        app['full_response'] = ''

    ## ä¿æŒæ¶ˆæ¯åœ¨èŠå¤©ä¸­
    for msg in app["messages"]:
        if msg["role"] == "user":
            st.chat_message(msg["role"], avatar="ğŸ§‘").write(msg["content"])
        elif msg["role"] == "assistant":
            st.chat_message(msg["role"], avatar="ğŸ¤–").write(msg["content"])

    # st.write(st.session_state)
    ## èŠå¤©
    if txt := st.chat_input():
        ### ç”¨æˆ·å†™å…¥
        app["messages"].append({"role": "user", "content": txt})
        st.chat_message("user", avatar="ğŸ§‘").write(txt)
        app["full_response"] = ""
    
        ### AI ä½¿ç”¨èŠå¤©æµå¼å“åº”
        # with st.chat_message("assistant", avatar="ğŸ¤–"):
        #     for chunk in ai.respond(app["messages"], use_knowledge=True):
        #         app["full_response"] += chunk
        #         st.write(chunk)

        ### AI éæµå¼å“åº”
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            chunk = ai.respond(app["messages"], use_knowledge=True)
            app["full_response"] += chunk["result"]
            st.write(app["full_response"])
        
            
        ### æ˜¾ç¤ºå†å²è®°å½•
        app["messages"].append({"role": "assistant", "content": app["full_response"]})
        app['history'].append("ğŸ§‘: " + txt)
        app['history'].append("ğŸ¤–: " + app["full_response"])
        st.sidebar.markdown("<br/>".join(app['history']) + "<br/><br/>", unsafe_allow_html=True)

class AI:
    def __init__(self):
        self.vectorstore = load_or_create_vectorstore()
        self.agent = build_agent(self.vectorstore)

    def respond(self, lst_messages, use_knowledge=False):
        # q = lst_messages[-1]["content"]
        # context = self.query(q)
        if use_knowledge:
            prompt = "Give the most accurate answer using your knowledge and the following information:"
        else:
            prompt = "Give the most accurate answer using only the following information:"
        res = self.agent.invoke({"query": lst_messages[-1]["content"]})
        return res

        # res_ai = ollama.chat(
        #     model="phi3",
        #     messages=[
        #         {"role": "system", "content": prompt},
        #         *lst_messages
        #     ],
        #     stream=True
        # )
        # for res in res_ai:
        #     chunk = res["message"]["content"]
        #     app["full_response"] += chunk
        #     yield chunk

ai = AI()
if __name__ == "__main__":
    main()
