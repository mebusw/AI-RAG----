##pip install python-dotenv langchain chromadb tiktoken sentence-transformers

import os
from dotenv import load_dotenv
import ollama
import base64
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import create_retrieval_chain, RetrievalQA
from langchain.callbacks.base import BaseCallbackHandler
from pydantic import BaseModel, Field
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
import requests
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
import logging
from openai import OpenAI
from langchain_core.embeddings import Embeddings
from typing import List
# logging.basicConfig(level=logging.DEBUG)

# Step 0: åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
API_URL = os.getenv("OPENAI_API_URL")
API_KEY = os.getenv("OPENAI_API_KEY")
CHAT_MODEL = os.getenv("CHAT_MODEL")
EMB_MODEL = os.getenv("EMB_MODEL")

print(f"{API_URL}, {API_KEY}, {CHAT_MODEL}, {EMB_MODEL}")

# Step 1.1 ä»PDFæ–‡æ¡£æèµ·æ–‡æœ¬
# å…ˆå°†æ–‡æ¡£è½¬æ¢ä¸ºå›¾åƒï¼Œç„¶åï¼Œä½¿ç”¨Tesseractè¯†åˆ«å›¾åƒä¸­çš„æ–‡æœ¬ã€‚Tesseractæ˜¯HPåœ¨1985å¹´å¼€å‘çš„ä¸»è¦OCRç³»ç»Ÿï¼Œç›®å‰ç”±è°·æ­Œç»´æŠ¤ã€‚
def handle_PDFs():
    import pdf2image #1.17.0
    doc_img = pdf2image.convert_from_path("data/doc_nvidia.pdf", dpi=300)

    import pytesseract #0.3.10
    doc_txt = []
    for page in doc_img:
        text = pytesseract.image_to_string(page)
        doc_txt.append(text)

# Step 1.2: åŠ è½½å’Œé¢„å¤„ç†æ–‡æœ¬
def load_documents():
    loader = TextLoader("csm_course_outline.txt")
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    split_docs = text_splitter.split_documents(documents)

    ## å¯é€‰åœ°ï¼Œä½¿ç”¨ç›®å½•ä¸ºæ®µè½æ·»åŠ æ ‡ç­¾
    def add_metadata(doc_txt=[]):
        title_map = { 
            "4-12" : "Business", 
            "13-33" : "Risk Factors",
            "34-44" : "Financials",
            "45-46" : "Directors",
            "47-83" : "Data" 
        }
        lst_docs, lst_ids, lst_metadata = [], [], []
        for n, page in enumerate(doc_txt):
            try:
                ## è·å–æ ‡é¢˜
                title = [v for k,v in title_map.items() if n in range(int(k.split("-")[0]), int(k.split("-")[1])+1)][0]
                ## æ¸…ç†é¡µé¢
                page = page.replace("Table of Contents","")
                ## è·å–æ®µè½
                for i,p in enumerate(page.split('\n\n')):
                    if len(p.strip())>5: 
                        lst_docs.append(p.strip())
                        lst_ids.append(str(n)+"_"+str(i))
                        lst_metadata.append({"title":title})
            except:
                continue
    
    ## å¯é€‰åœ°ï¼Œå…ƒæ•°æ®å¢å¼ºå¯ä»¥æ˜¾è‘—æé«˜æ–‡æ¡£æ£€ç´¢æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨Phi3å°†æ¯ä¸ªæ®µè½æ€»ç»“ä¸ºå‡ ä¸ªå…³é”®è¯ã€‚
    def keyword_generator(p, top=3):
        prompt = "summarize the following paragraph in 3 keywords separated by , : "
        res = ollama.generate(model="phi3", prompt=prompt)["response"]
        return res.replace("\n", " ").strip()

    
    return split_docs

# Step 1.3: åŠ è½½å’Œé¢„å¤„ç†å›¾åƒå›¾è¡¨
# æœ¬å‡½æ•°æ˜¯è°ƒç”¨ollamaæŸä¸ªæ¨¡å‹æ¥æè¿°å›¾åƒæˆ–å›¾è¡¨
def load_images():
    def encode_image(path):
        with open(path, "rb") as file:
            return base64.b64encode(file.read()).decode('utf-8')

    img = encode_image("/path/to/image.jpg")
    prompt = "describe the image"
    res = ollama.generate(model="llava", prompt=prompt, images=[img])["response"] ##æ³¨æ„è¿™é‡Œçš„æç¤ºè¯å‚æ•°æ˜¯promptï¼Œä¸æ˜¯messages
    print(res)

    img = encode_image("/path/to/diagram.jpg")
    prompt = "Describe the image in detail. Be specific about graphs, such as bar plots, line graphs, etc."
    res = ollama.generate(model="llava", prompt=prompt, images=[img])["response"] ##æ³¨æ„è¿™é‡Œçš„æç¤ºè¯å‚æ•°æ˜¯promptï¼Œä¸æ˜¯messages
    print(res)


# Step 2: åŠ è½½æˆ–åˆ›å»ºå‘é‡æ•°æ®åº“
class CustomOpenAICompatibleEmbeddings(Embeddings):
    """
    Custom Embeddings class to interact directly with an OpenAI-compatible API,
    bypassing Langchain's default model validation.
    """
    def __init__(self, api_key: str, base_url: str, model: str):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model_name = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents."""
        # Ensure the input is a list of strings, as expected by OpenAI client
        response = self.client.embeddings.create(input=texts, model=self.model_name)
        print(f"Embedding doc response: {response}")
        return [data.embedding for data in response.data]

    def embed_query(self, text: str) -> List[float]:
        """Embed a single query string."""
        # Ensure the input is a list containing the single string
        response = self.client.embeddings.create(input=[text], model=self.model_name)
        return response.data[0].embedding

def load_or_create_vectorstore():
    embeddings = CustomOpenAICompatibleEmbeddings(
                    api_key=API_KEY, 
                    model=EMB_MODEL, 
                    base_url=API_URL, 
                )

    if os.path.exists("local_db"):
        print("Loading existing vector database...")
        return Chroma(persist_directory="local_db/", embedding_function=embeddings)
    else:
        print("Creating new vector database...")
        documents = load_documents()
        return Chroma.from_documents(documents, embeddings, persist_directory="local_db/")

from typing import Generator
from queue import Queue

class StreamingCallbackHandler(BaseCallbackHandler):
    def on_llm_new_token(self, token: str, **kwargs):
        self.queue.put(token)

    def __init__(self):
        self._generator = None
        self.queue = Queue()

    def start_streaming(self) -> Generator[str, None, None]:
        """
        å¯åŠ¨ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œç”¨äºæ¶ˆè´¹ on_llm_new_token ä¸­çš„æ•°æ®ã€‚
        """
        if self._generator is not None:
            try:
                while True:
                    token = self.queue.get(block=True) 
                    if token is None:  # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                        break
                    yield token
            except StopIteration:
                # ç”Ÿæˆå™¨å·²è¢«å…³é—­
                pass

    def set_generator(self, generator: Generator[str, None, None]):
        """
        è®¾ç½®ä¸€ä¸ªç”Ÿæˆå™¨å®ä¾‹ã€‚
        """
        self._generator = generator

handler = StreamingCallbackHandler()


# Step 3: æ„å»ºä»£ç†
def build_agent(vectorstore, streaming=False):
    retriever = vectorstore.as_retriever()
    llm = ChatOpenAI(temperature=0, api_key=API_KEY, model=CHAT_MODEL, base_url=API_URL,
                streaming=streaming,
                callbacks=[handler],)
    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
    return qa_chain
    
# Step 4: ä¸»å‡½æ•°ï¼Œè¿è¡Œä»£ç†
def main():

    def _main_loop():
        vectorstore = load_or_create_vectorstore()
        agent = build_agent(vectorstore)
        while True:
            print("\nAI Agent is ready. Type your questions (type 'exit' to quit).")
            query = input(">>> You: ")
            if query.lower() == "exit":
                break
            
            result = agent.invoke({"query": query})
            print(">>> AI: ", result["result"])
            print("Sources:", [doc.metadata["source"] for doc in result["source_documents"]])
    _main_loop()


# Step 4A: Streamlitæ˜¯æ„å»ºå¿«é€ŸWebåº”ç”¨ç¨‹åºæœ€å¸¸ç”¨çš„Pythonåº“ï¼Œå› ä¸ºå®ƒé€šè¿‡å…¶æµå¼åŠŸèƒ½ç®€åŒ–äº†NLPåº”ç”¨ç¨‹åºçš„å¼€å‘ã€‚é¦–å…ˆï¼Œå®šä¹‰å¸ƒå±€ï¼šæˆ‘çš„å±å¹•åº”æœ‰ä¸€ä¸ªä¾§è¾¹æ ï¼Œç”¨æˆ·å¯ä»¥åœ¨å…¶ä¸­æŸ¥çœ‹èŠå¤©å†å²è®°å½•ã€‚
import streamlit as st
def buildUI():
    ## å¸ƒå±€
    st.title("Write your questions")
    st.sidebar.title("Chat History")

    app = st.session_state
    if 'messages' not in app:
        app['messages'] = [{"role": "assistant", "content": "I'm ready to retrieve information"}]
    if 'history' not in app:
        app['history'] = []
    if 'full_response' not in app:
        app['full_response'] = ''

    ## ä¿æŒæ¶ˆæ¯åœ¨èŠå¤©ä¸­
    for msg in app["messages"]:
        if msg["role"] == "user":
            st.chat_message(msg["role"], avatar="ğŸ§‘").write(msg["content"])
        elif msg["role"] == "assistant":
            st.chat_message(msg["role"], avatar="ğŸ¤–").write(msg["content"])

    ## èŠå¤©
    if txt := st.chat_input():
        ### ç”¨æˆ·å†™å…¥
        app["messages"].append({"role": "user", "content": txt})
        st.chat_message("user", avatar="ğŸ§‘").write(txt)
        app["full_response"] = ""
    
        # ### AI ä½¿ç”¨èŠå¤©æµå¼å“åº”
        # with st.chat_message("assistant", avatar="ğŸ¤–"):
        #     ai.respond(app["messages"], use_knowledge=True)
        #     for chunk in handler._generator:
        #         print(f"chunk={chunk}")
        #         if chunk is not None:
        #             st.write(chunk) #FIXME æµå¼ä¸‹ï¼Œst.writeæ²¡åŠæ³•ä¸æ¢è¡Œ
        #             app["full_response"] += str(chunk)

        ### AI éæµå¼å“åº”
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            chunk = ai.respond(app["messages"], use_knowledge=True)
            app["full_response"] += chunk["result"]
            st.write(app["full_response"])
            print("Sources:", [doc.metadata["source"] for doc in chunk["source_documents"]])


        ### æ˜¾ç¤ºå†å²è®°å½•
        app["messages"].append({"role": "assistant", "content": app["full_response"]})
        app['history'].append("ğŸ§‘: " + txt)
        app['history'].append("ğŸ¤–: " + app["full_response"])
        st.sidebar.markdown("<br/>".join(app['history']) + "<br/><br/>", unsafe_allow_html=True)


class AI:
    def __init__(self):
        self.vectorstore = load_or_create_vectorstore()
        self.agent = build_agent(self.vectorstore, streaming=False)
        # åˆ›å»ºç”Ÿæˆå™¨å¹¶å¯åŠ¨
        generator = handler.start_streaming()
        handler.set_generator(generator)


    def respond(self, lst_messages, use_knowledge=False):
        if use_knowledge:
            prompt = "Give the most accurate answer using your knowledge to user's query.\n'{query}':"
        else:
            prompt = "Give the most accurate answer without external knowledge to user's query.\n'{query}':"

        res = self.agent.invoke({"query": prompt + lst_messages[-1]["content"]})
        return res


if __name__ == "__main__":
    ai = AI()
    buildUI()
    # main()
